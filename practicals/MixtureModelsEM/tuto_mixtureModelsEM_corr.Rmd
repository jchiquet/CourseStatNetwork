---
title: "Tutorial: Reminder (?) on mixture model and the EM algorithm - correction"
author: "MSc in Statistics for Smart Data -- Introduction to graph analysis and modeling"
date: "Julien Chiquet, November the 13th, 2018"
fontsize: 11pt
lang: en
geometry: left=1.45in,top=1.35in,right=1.45in,bottom=1.35in
classoption: a4paper
linkcolor: red
urlcolor: blue
citecolor: green
output:
  pdf_document:
    number_sections: true
    citation_package: natbib
    includes:
      in_header: ../preamble.sty

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = FALSE)
set.seed(1234)
```

# Gaussian Mixture Models

We    consider   a   collection   of    random   variables $(X_1, \dots,  X_n)$ associated  with $n$  individuals drawn from $Q$ populations. The label of each individual describes the population (or class) to  which it belongs and  is unobserved.  The $Q$  classes have _a priori_ distribution ${\boldsymbol     \alpha} = (\alpha_1,\dots,\alpha_Q)$ with
$\alpha_q  = \mathbb{P}(i  \in  q)$.  The  hidden  random indicator  variables $(Z_{iq})_{i\in\mathcal{P},  q\in\mathcal{Q}}$ describe  the label  of each individuals, that is,

\begin{equation*}
  \label{eq:prior_classes}
  \alpha_q  = \mathbb{P}(Z_{iq}  =  1) =  \mathbb{P}(i  \in q),  \quad
  \text{ such that} \sum_{q=1}^Q \alpha_q = 1.
\end{equation*}
Remark that we have $\mathbf{Z}_i      =      (Z_{i1},       \dots,      Z_{iQ})      \sim
\mathcal{M}(1,\boldsymbol\alpha)$. The distribution of  $X_i$ conditional on the label of  $i$ is assumed
to be a univariate gaussian distribution with unknown parameters, that
is, $X_i | Z_{iq} = 1 \sim \mathcal{N}(\mu_q,\sigma^2_q)$.

# Questions

## _Likelihood._ Write the model complete-data loglikelihood.

We denote the vector of parameters to be estimated by
$\mathbf{\mu} = (\mu_1,\dots,\mu_Q)$, $\mathbf{\sigma^2} =
(\sigma^2_1,\dots,\sigma^2_Q)$, $\boldsymbol\tau = (\tau_{iq,
i=1,\dots,n; q=1,\dots Q})$. The negative complete-data
loglikelihood is derived as follows

\Answer{%
\begin{equation}
\begin{aligned}
\log L(\boldsymbol{\mu},\boldsymbol{\sigma}^2,\boldsymbol\tau; \mathbf{x}) & = \sum_{i=1}^n \log \left(\mathbb{P}rod_{q=1}^Q \alpha_q f(x_i; \mu_q,\sigma^2_q)^{Z_{iq}} \right)\\
& = \sum_{i=1}^n \sum_{q=1}^Q Z_{iq} \left(\log  \frac{\alpha_q}{\sigma\sqrt{2\mathbb{P}i}} \exp\{\frac{1}{2\sigma^2}(x_i - \mu_q)^2\} \right)\\
& = \sum_{i=1}^n \sum_{q=1}^Q Z_{iq} \left(\log \alpha_q - \log\sigma -\log(\sqrt{2\mathbb{P}i}) - \frac{1}{2\sigma_q^2} (x_i - \mu_q)^2 \right)
\end{aligned}
\end{equation}
}

## _E-step._ 

For fixed values of  $\hat\mu_q, \hat\sigma_q^2$
and  $\hat\alpha_q$, give  the expression  of the  estimates of  the
posterior probabilities $\tau_{iq}= \mathbb{P}(Z_{iq}=1|X_i)$.

\Answer{
\begin{equation}
\tau_{iq} = \frac{\hat\alpha_q f(x_i; \hat{\mu}_q, \hat\sigma_q^2)}{\sum_{q=1}^Q \hat\alpha_q f(x_i; \hat{\mu}_q, \hat\sigma_q^2)},
\end{equation}
where $f$ is the density of the normal distribution.
}

## _M-step._ 

\Answer{ The maximization step consists in solving the following optimization problem
\begin{equation}
\label{eq:m_step_optim}
\arg \max_{\sigma_q,\mu_q,\alpha_q}  \underbrace{\sum_{i=1}^n \sum_{q=1}^Q \hat\tau_{iq} \left(\log \alpha_q - \log\sigma -\log(\sqrt{2\mathbb{P}i}) - \frac{1}{2\sigma_q^2} (x_i - \mu_q)^2 \right)}_{Q(\boldsymbol\sigma,\boldsymbol\mu,{\boldsymbol\alpha};\hat{\boldsymbol\tau})}
\end{equation}

Consider first the mixture coefficients. We solve the above
maximization problem under the constraint that the mixture
coefficients sum to 1. This can be dealt with the Lagrange
multiplier technique. By deriving the objective function w.r.t
$\alpha_q$, we get

\begin{equation} 
\frac{\sum_i\tau_{iq}}{\alpha_q} + \lambda = 0 \Leftrightarrow  \alpha_q = \frac{\sum_i\tau_{iq}}{-\lambda}  
\end{equation}

where $\lambda$ corresponds to the Lagrange multiplier
associated with the constraint $\sum_q \alpha_q=1$. If we sum
the latter result over all $q$, we get that $1 = \sum_q
\tau_{iq} / (-\lambda)$. In other words, $\lambda = -\sum_q
\tau_{iq}$ so that finally
\begin{equation} 
\hat\alpha_q = \frac{\sum_{i=1}^n \tau_{iq}}{\sum_{i=1}^n\sum_{q=1}^Q \tau_{iq}}  
\end{equation}

Concerning, $\hat\mu_q$, null gradient condition leads to 
\begin{equation} 
\sum_i \frac{\tau_{iq}}{2\sigma_q^2}(x_i-\mu_q)  = 0 \Leftrightarrow  \mu_q = \frac{\sum_i \tau_{iq} x_i}{\sum_i \tau_{iq}}
\end{equation}

Similarly, for $\hat\sigma_q$, we get
\begin{equation} 
\sum_{i=1}^n \tau_{iq} \left(-\frac{1}{2\sigma_q^2} + \frac{1}{2\sigma_q^4}(x_i-\mu_q)^2 \right)  = 0 \Leftrightarrow  \sigma^2_q = \frac{\sum_{i=1}^n \tau_{iq} (x_i-\mu_q)^2}{\sum_{i=1}^n \tau_{iq}}
\end{equation}

}

- _Implementation._  

```{r EM}
get.cloglik <- function(X, Z, alpha, mu, sigma) {
    n <- length(X); Q <- length(alpha)
    xs <- sweep(sweep(matrix(X,n,Q),2,mu,"-"),2,sigma,"/")
    return(sum(t(Z)*(log(alpha)-log (sigma)-.5*(log(2*pi)+t(xs)^2))))
}

EM.mixture <- function(X, Q, 
                       init.cl=kmeans(X,Q)$cl, max.iter=100, eps=1e-5) {
    ## INITIALIZATIONS
    n <- length(X)
    tau <- matrix(0,n,Q)
    tau[cbind(1:n,init.cl)] <- 1
    loglik  <- vector("numeric", max.iter)
    cloglik <- vector("numeric", max.iter)
    iter <- 0; cond <- FALSE

    while (!cond) {
        iter <- iter + 1
        ## M step
        alpha  <- colMeans(tau)
        mu     <- colMeans(tau * matrix(X,n,Q)) / alpha
        sigma  <- sqrt(colMeans(tau*sweep(matrix(X,n,Q),2,mu,"-")^2)/alpha)
        ## E step    
        prob <- sapply(1:Q, function(q) alpha[q]*dnorm(x,mu[q],sigma[q]))
        likelihoods <- rowSums(prob)
        tau <-  prob / likelihoods 
        Z <- 1*(tau >= .5)
        ## check consistency
        loglik[iter]  <- sum(log(likelihoods))
        cloglik[iter] <- get.cloglik(X, Z, alpha, mu, sigma)
        if (iter > 1) 
            cond <- (iter>=max.iter) | abs(cloglik[iter]-cloglik[iter-1]) < eps        
    }

    return(list(alpha = alpha , 
                mu    = mu    , 
                sigma = sigma, 
                tau   = tau   ,
                cl    = apply(Z,1,which.max),
                cloglik = cloglik[1:iter],
                loglik  = loglik[1:iter]))
}
```

## Examples

We test ICL and BIC on a simple example with 4 groups

Let us start with the data generation.

```{r data generation}
mu1 <- 5   ; sigma1 <- 1; n1 <- 100
mu2 <- 10  ; sigma2 <- 1; n2 <- 200
mu3 <- 15  ; sigma3 <- 2; n3 <- 50
mu4 <- 20  ; sigma4 <- 3; n4 <- 100
cl <- rep(1:4,c(n1,n2,n3,n4))
x <- c(rnorm(n1,mu1,sigma1),rnorm(n2,mu2,sigma2),
       rnorm(n3,mu3,sigma3),rnorm(n4,mu4,sigma4))
n <- length(x)

## we randomize the class ordering
rnd <- sample(1:n)
cl <- cl[rnd]
x  <- x[rnd]

alpha <- c(n1,n2,n3,n4)/n

curve(alpha[1]*dnorm(x,mu1,sigma1) +
      alpha[2]*dnorm(x,mu2,sigma2) +
      alpha[3]*dnorm(x,mu3,sigma3) +
      alpha[4]*dnorm(x,mu4,sigma3), 
      col="blue", lty=1, from=0,to=30, n=1000,
      main="Theoretical Gaussian mixture and its components",
      xlab="x", ylab="density")
curve(alpha[1]*dnorm(x,mu1,sigma1), col="red", add=TRUE, lty=2)
curve(alpha[2]*dnorm(x,mu2,sigma2), col="red", add=TRUE, lty=2)
curve(alpha[3]*dnorm(x,mu3,sigma3), col="red", add=TRUE, lty=2)
curve(alpha[4]*dnorm(x,mu4,sigma4), col="red", add=TRUE, lty=2)
rug(x)
```

Suppose that we know the number of components, i.e. 4.

```{r test EM}
out <- EM.mixture(x, Q=4)
plot(out$loglik, main="data log-likelihood", type="l")

curve(alpha[1]*dnorm(x,mu1,sigma1) +
      alpha[2]*dnorm(x,mu2,sigma2) +
      alpha[3]*dnorm(x,mu3,sigma3) +
      alpha[4]*dnorm(x,mu4,sigma3), col="blue",
      lty=1, from=0,to=30, n=1000,
      main="Theoretical Gaussian mixture and estimated components",
      xlab="x", ylab="density")
curve(out$alpha[1]*dnorm(x,out$mu[1],out$sigma[1]), col="red", add=TRUE, lty=2)
curve(out$alpha[2]*dnorm(x,out$mu[2],out$sigma[2]), col="red", add=TRUE, lty=2)
curve(out$alpha[3]*dnorm(x,out$mu[3],out$sigma[3]), col="red", add=TRUE, lty=2)
curve(out$alpha[4]*dnorm(x,out$mu[4],out$sigma[4]), col="red", add=TRUE, lty=2)
rug(x)

## the confusion table gives rather good results
table(out$cl,cl)
```

The number of component mixture is hard to recover because of the last two mixed components

```{r model selection}
seq.Q <- 2:10
crit.EM <- sapply(seq.Q, function(Q) {
    out <- EM.mixture(x, Q)
    df <- Q-1 + 2 * Q
    return(c(BIC = -2*tail(out$loglik,1)  + log(n)*df,
             ICL = -2*tail(out$cloglik,1) + log(n)*df ))
})
matplot(seq.Q, t(crit.EM), type="l", col=c("red", "blue"))
legend("topleft", c("BIC", "ICL"), col=c("red", "blue"),lty=1)

Q.hat <- seq.Q[which.min(crit.EM[1, ])]
out <- EM.mixture(x, Q=Q.hat)
par(mfrow=c(1,1))
curve(alpha[1]*dnorm(x,mu1,sigma1) +
      alpha[2]*dnorm(x,mu2,sigma2) +
      alpha[3]*dnorm(x,mu3,sigma3) +
      alpha[4]*dnorm(x,mu4,sigma3), col="blue", 
      lty=1, from=0,to=30, n=1000,
      main="Theoretical Gaussian mixture and estimated components",
      xlab="x", ylab="density")
for (q in 1:Q.hat) {
    curve(out$alpha[q]*dnorm(x,out$mu[q],out$sigma[q]), col="red", add=TRUE, lty=2)
}
rug(x)
```
